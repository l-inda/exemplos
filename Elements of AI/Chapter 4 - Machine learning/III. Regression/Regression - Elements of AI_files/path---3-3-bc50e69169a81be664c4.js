webpackJsonp([59403690584698],{1479:function(e,t){e.exports={data:{markdown:{htmlAst:{type:"root",children:[{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"lead",properties:{},children:[{type:"text",value:"One of the most useful applications of the Bayes rule is the so-called naive Bayes classifier. "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The Bayes classifier is a machine learning technique that can be used to classify objects such as text documents into two or more classes. The classifier is trained by analysing a set of training data, for which the correct classes are given."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The naive Bayes classifier can be used to determine the probabilities of the classes given a number of different observations. The assumption in the model is that the feature variables are conditionally independent given the class. (We will not discuss the meaning of conditional independence in this course. For our purposes, it is enough to be able to exploit conditional independence in building the classifier.)"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Real world application: spam filters"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We will use a spam email filter as a running example for illustrating the idea of the naive Bayes classifier. Thus, the class variable indicates whether a message is spam (or “junk email”) or whether it is a legitimate message (also called “ham”). The words in the message correspond to the feature variables, so that the number of feature variables in the model is determined by the length of the message."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Why we call it “naive”",description:"Using spam filters as an example, the idea is to think of the words as being produced by choosing one word after the other so that the choice of the word depends only on whether the message is spam or ham. This is a crude simplification of the process because it means that there is no dependency between adjacent words, and the order of the words has no significance. This is in fact why the method is called naive."},children:[{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The above idea is usually depicted using the following illustration where the class of the message (spam or ham) is the only factor that has an effect on the words."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"spam-or-ham"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Despite it’s naivete, the naive Bayes method tends to work very well in practice. This is a good example of what the common saying in statistics, “all models are wrong, but some are useful” means. (The aphorism is generally attributed to statistician "},{type:"element",tagName:"a",properties:{href:"https://en.wikipedia.org/wiki/George_E._P._Box"},children:[{type:"text",value:"George E.P. Box"}]},{type:"text",value:".)"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Estimating parameters"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To get started, we need to specify the prior odds for spam (against ham). For simplicity assume this to be 1:1 which means that on the average half of the incoming messages are spam. (In reality, the amount of spam is probably much higher.)"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To get our likelihood ratios, we need two different probabilities for any word occurring: one in spam messages and another one in ham messages."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The word distributions for the two classes are best estimated from actual training data that contains some spam messages as well as legitimate messages. The simplest way is to count how many times each word, abacus, acacia, ..., zurg, appears in the data and divide the number by the total word count."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To illustrate the idea, let’s assume that we have at our disposal some spam and some ham. You can easily obtain such data by saving a batch of your emails in two files."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Assume that we have calculated the number of occurrences of the following words (along with all other words) in the two classes of messages:"}]},{type:"text",value:"\n"},{type:"element",tagName:"table",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"tbody",properties:{},children:[{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"word"}]},{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"spam"}]},{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"ham"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"million"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"156"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"98"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"dollars"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"29"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"119"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"adclick"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"51"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"conferences"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"12"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"total"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"95791"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"306438"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We can now estimate that the probability that a word in a spam message is million, for example, is about 156 out of 95791, which is roughly the same as 1 in 614. Likewise, we get the estimate that 98 out of 306438 words, which is about the same as 1 in 3127, in a ham message are million. Both of these probability estimates are small, less than 1 in 500, but more importantly, the former is higher than the latter: 1 in 614 is higher than 1 in 3127. This means that the likelihood ratio, which is the first ratio divided by the second ratio, is more than one. To be more precise, the ratio is (1/614) / (1/3127) = 3127/614 = 5.1 (rounded to one decimal digit)."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Recall that if you have any trouble at all with following the math in this section, you should refresh the arithmetics with fractions using the pointers we gave earlier (see the part about Odds in section "},{type:"element",tagName:"i",properties:{},children:[{type:"text",value:"Odds and Probability"}]},{type:"text",value:")."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Zero means trouble",description:"One problem with estimating the probabilities directly from the counts is that the zero counts lead to zero estimates. This can be quite harmful for the performance of the classifier - it easily leads to situations where the posterior odds is 0/0, which is nonsense. The simplest solution is to use a small lower bound for all probability estimates. The value 1/100000, for instance, does the job."},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Using the above logic, we can determine the likelihood ratio for all possible words without having to use zero, giving us the following likelihood ratios:"}]},{type:"text",value:"\n"},{type:"element",tagName:"table",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"tbody",properties:{},children:[{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"word"}]},{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"likelihood ratio"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"million"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"5.1"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"dollars"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0.8"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"adclick"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"53.2"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"conferences"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0.3"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We are now ready to apply the method to classify new messages."}]},{type:"text",value:"\n"},{type:"element",tagName:"h3",properties:{},children:[{type:"text",value:"Example: is it spam or ham?"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Once we have the prior odds and the likelihood ratios calculated, we are ready to apply the Bayes rule, which we already practiced in the medical diagnosis case as our example. The reasoning goes just like it did before: we update the odds of spam by multiplying it by the likelihood ratio. To remind ourselves of the procedure, let's try a message with a single message to begin with. For the prior odds, as agreed above, you should use odds 1:1."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec62d906ee0000047c59a0"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To handle the rest of the words in a message, we can use exactly the same procedure. The posterior odds after one word, which you calculated in the previous exercise, will become the prior odds for the next word, and so on."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec630a06ee0000047c59a2"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Hooray! You have now mastered a powerful technique used every day in a wide range of real-world AI applications, the naive Bayes classifier. Even if you had to skip some of the technicalities, you should try to make sure you understood the basic principles of applying probabilities to update beliefs. As we discussed in the beginning of this Chapter, the main advantage of probabilistic reasoning is the ability to handle uncertain and conflicting evidence. Using examples in medical diagnosis and spam filtering, we demonstrated how this work is practice."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"part-summary",properties:{chapter:"3",heading:"After completing Chapter 3 you should be able to:",listitems:'[\n  {"content":"Express probabilities in terms of natural frequencies"},\n  {"content":"Apply the Bayes rule to infer risks in simple scenarios"},\n  {"content":"Explain the base-rate fallacy and avoid it by applying Bayesian reasoning"}\n    ]'},children:[{type:"text",value:">\n  "}]},{type:"text",value:"\n"}]}],data:{quirksMode:!1}},frontmatter:{path:"/3/3",title:"Naive Bayes classification",section:3,part:3,lang:"en"}},allRelatedSections:{totalCount:3,edges:[{node:{htmlAst:{type:"root",children:[{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"lead",properties:{},children:[{type:"text",value:"In the previous section, we discussed search and it’s application where there is perfect information – such as in games like chess. However, in the real world things are rarely so clear cut. "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Instead of perfect information, there is a host of unknown possibilities, ranging from missing information to deliberate deception."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Take a self-driving car for example — you can set the goal to get from A to B in an efficient and safe manner that follows all laws. But what happens if the traffic gets worse than expected, maybe because of an accident ahead? Sudden bad weather? Random events like a ball bouncing in the street, or a piece of trash flying straight into the car’s camera?"}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"self-driving-car",color:"#e9e9ed",frombottom:"0%",totalheight:"50%"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"A self-driving car needs to use a variety of sensors, including sonar-like ones and cameras, to detect where it is and what is around it. These sensors are never perfect as the data from the sensors always includes some errors and inaccuracies called “noise”. It is very common then that one sensor indicates that the road ahead turns left, but another sensor indicates the opposite direction. This needs to be resolved without always stopping the car in case of even a slightest amount of noise."}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Probability"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:'One of the reasons why modern AI methods actually work in real-world problems - as opposed to most of the earlier “good old-fashioned" methods in the 1960-1980s - is their ability to deal with uncertainty.'}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"The history of dealing with uncertainty",description:"The history of AI has seen various competing paradigms for handling uncertain and imprecise information. For example, you may have heard of fuzzy logic. Fuzzy logic was for a while a contender for the best approach to handle uncertain and imprecise information and used in many customer-applications such as washing machines where the machine could detect the dirtiness (a matter of degrees, not only dirty or clean) and adjust the program accordingly.<br><br>However, probability has turned out to be the best approach for reasoning under uncertainty, and almost all current AI applications are based, in at least some degree, on probabilities."},children:[{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"poker"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Why probability matters"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We are perhaps most familiar with applications of probability in games: what are the chances of getting three of a kind in poker (about one in 46), what are the chances of winning in the lottery (very small), and so on. However, far more importantly, probability can also be used to quantify and compare risks in everyday life: what are the chances of crashing your car if you exceed the speed limit, what are the chances that the interest rates on your mortgage will go up by five percentage points within the next five years, or what are the chances that AI will automate particular tasks such as detecting fractured bones in X-ray images or waiting tables in a restaurant."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"The key lesson about probability",description:"The most important lesson about probability that we’d like you to take away is not probability calculus. Instead, it is the ability to think of uncertainty as a thing that can be quantified at least in principle. This means that we can talk about uncertainty as if it were a number: numbers can be compared (“is this thing more probable than that thing”), and they can often be measured.<br><br>Granted, measuring probabilities is hard: we usually need many observations about a phenomenon to draw conclusions. However, by systematically collecting data, we can critically evaluate probabilistic statements, and our numbers can sometimes be found to be right or wrong. In other words, the key lesson is that uncertainty is not beyond the scope of rational thinking and discussion, and probability provides a systematic way of doing just that."},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The fact that uncertainty can be quantified is of paramount importance, for example, in decision concerning vaccination or other public policies. Before entering the market, any vaccine is clinically tested, so that its benefits and risks have been quantified. The risks are never known to the minutest detail, but their magnitude is usually known to sufficient degree that it can be argued whether the benefits outweigh the risks."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Why quantifying uncertainty matters",description:"If we think of uncertainty as something that can’t be quantified or measured, the uncertainty aspect may become an obstacle for rational discussion. We may for example argue that since we don’t know exactly whether a vaccine may cause a harmful side-effect, it is too dangerous to use. However, this may lead us to ignore a life-threatening disease that the vaccine will eradicate. In most cases, the benefits and risks are known to sufficient precision to clearly see that one is more significant than the other."},children:[{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The above lesson is useful in many everyday scenarios and professionally: for example, medical doctors, judges in a court of law, investors have to process uncertain information and make rational decisions based on them. Since this is an AI course, we will discuss how probability can be used to automate uncertain reasoning. The examples we will use include medical diagnosis (although it is usually not a task that we’d wish to fully automate), and identifying fraudulent email messages (“spam”)."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec5e7c06ee0000047c5995"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Odds"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Probably the easiest way to represent uncertainty is through odds. They make it particularly easy to update beliefs when more information becomes available (we will return to this in the next section)."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Before we proceed any further, we should make sure you are comfortable with doing basic manipulations on ratios (or fractions). As you probably recall, fractions are numbers like 3/4 or 21/365. We will need to multiply and divide such things, so it's good to refresh these operations if you feel unsure about them. A compact presentation for those who just need a quick reminder is "},{type:"element",tagName:"a",properties:{href:"https://en.wikibooks.org/wiki/Arithmetic/Multiplying_Fractions"},children:[{type:"text",value:"Wikibooks: Multiplying Fractions"}]},{type:"text",value:". Another fun animated presentation of the basic operations is "},{type:"element",tagName:"a",properties:{href:"https://www.mathsisfun.com/algebra/rational-numbers-operations.html"},children:[{type:"text",value:"Math is Fun: Using Rational Numbers"}]},{type:"text",value:". Feel free to consult your favourite source if necessary."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"By odds, we mean for example 3:1 (three to one), which means that we expect that for every three cases of an outcome, for example winning a bet, there is one case of the opposite outcome, not winning the bet. The other way to express the same would be to say that the chances of winning are 3/4 (three in four). These are called natural frequencies since they involve only whole numbers. With whole numbers, it is easy to imagine, for example, four people out of whom, three have brown eyes. Or four days out of which it rains on three (if you’re in Helsinki)."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"eyes"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Why we use odds and not percentages",description:"Three out of four is of course the same as 75%. (Mathematicians prefer to use fractions like 0.75 instead of percentages.) It has been found that people get confused and make mistakes more easily when dealing with fractions and percentages than with natural frequencies or odds. This is why we use natural frequencies and odds whenever convenient."},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"An important thing to notice is that while expressed as two numbers, 3 and 1 for example, the odds can actually be thought of as a single fraction or a ratio, for example 3/1 (three divided by one) which the equal to 3. Thus, the odds 3:1 is the same as the odds 6:2 or 30:10 since these ratios are also equal to 3. Likewise, the odds 1:5 can be thought of as 1/5 (one divided by five) which equals 0.2. Again, this is the same as the odds 2:10 or 10:50 because that's what you get by dividing 2 by 10 or 10 by 50. But be very careful! The odds 1:5 (one win for every five losses), even if it can be expressed as the decimal number 0.2, is different from 20% probability (or probability 0.2 using the mathematicians' notation). The odds 1:5 mean that you'd have to play the game six times to get one win on the average. The probability 20% means that you'd have to play five times to get one win on the average."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"For odds that are greater than one, such as 5:1, it is easy to remember that we are not dealing with probabilities because no probability can be greater than 1 (or greater than 100%), but for odds that are less than one such as 1:5, the danger of confusion lurks around the corner."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"So make sure you always know when we are talking about odds and when we are talking about probabilities."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The following exercise will help you practice dealing with correspondence between odds and probabilities. Don't worry if you make some mistakes at this stage: the main goal is to learn the skills that you will need in the next sections."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec609506ee0000047c5998"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]}],data:{quirksMode:!1}},excerpt:"Instead of perfect information, there is a host of unknown possibilities, ranging from missing information to deliberate deception. Take a…",frontmatter:{path:"/3/1",title:"Odds and probability",part:3,type:"section",lang:"en",section:1}}},{node:{htmlAst:{type:"root",children:[{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"lead",properties:{},children:[{type:"text",value:"We will not go too far into the details of probability calculus and all the ways in which it can be used in various AI applications. But we will discuss one very important formula. "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We will do this because this particular formula is both simple and elegant as well as incredibly powerful. It can be used to weigh conflicting pieces of evidence in medicine, in a court of law, and in many (if not all) scientific disciplines. "},{type:"element",tagName:"strong",properties:{},children:[{type:"text",value:"The formula is called the Bayes rule (or the Bayes formula)."}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We will start by demonstrating the power of the Bayes rule by means of a simple medical diagnosis problem where it highlights how poorly our intuition is suited for combining conflicting evidence. We will then show how the Bayes rule can be used to build AI methods that can cope with conflicting and noisy observations."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"key-terminology",properties:{terminologies:'[\n      {\n      "title":"Prior and posterior odds",\n      "content":"The Bayes rule can be expressed in many forms. The simplest one is in terms of odds. The idea is to take the odds for something happening (against it not happening), which we´ll write as prior odds. The word prior refers to our assessment of the odds before obtaining some new information that may be relevant. The purpose of the formula is to update the prior odds when new information becomes available, to obtain the posterior odds, or the odds after obtaining the information. (The dictionary meaning of posterior is “something that comes after, later.“)"}\n  ]'},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"oddchange"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"How odds change"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"In order to weigh the new information, and decide how the odds change when it becomes available, we need to consider how likely we would be to encounter this information in alternative situations. Let’s take as an example, the odds that it will rain later today. Imagine getting up in the morning in Finland. The chances of rain are 206 in 365. (Including rain, snow, and hail. Brrr!) The number of days without rain is therefore 159. This converts to prior odds of 206:159 for rain, so the cards are stacked against you already before you open your eyes."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"However, after opening your eyes and taking a look outside, you notice it’s cloudy. Suppose the chances of having a cloudy morning on a rainy day are 9 out of 10 — that means that only one out of 10 rainy days start out with blue skies. But sometimes there are also clouds without rain: the chances of having clouds on a rainless day are 1 in 10. Now how much higher are the chances of clouds on a rainy day compared to a rainless day? Think about this carefully as it will be important to be able to comprehend the question and obtain the answer in what follows."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The answer is that the chances of clouds are "
},{type:"element",tagName:"strong",properties:{},children:[{type:"text",value:"nine times"}]},{type:"text",value:" higher on a rainy day than on a rainless day: on a rainy day the chances are 9 out of 10, whereas on a rainless day the chances of clouds are 1 out of 10, and that makes nine times higher."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"key-terminology",properties:{terminologies:'[\n      {"title":"Likelihood ratio","content":"The above ratio (nine times higher chance of clouds on a rainy day than on a rainless day) is called the likelihood ratio. More generally, the likelihood ratio is the probability of the observation in case the event of interest (in the above, rain), divided by the probability of the observation in case of no event (in the above, no rain). Please read the previous sentence a few times. It may look a little intimidating, but it´s not impossible to digest if you just focus carefully. We will walk you through the steps in detail, just don´t lose your nerve. We´re almost there."}\n  ]'},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"So we concluded that on a cloudy morning, we have: "},{type:"element",tagName:"strong",properties:{},children:[{type:"text",value:"likelihood ratio = (9/10) / (1/10) = 9"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The mighty Bayes rule for converting prior odds into posterior odds is — ta-daa! — as follows: "},{type:"element",tagName:"strong",properties:{},children:[{type:"text",value:"posterior odds = likelihood ratio × prior odds"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Now you are probably thinking: Hold on, that’s the formula? It’s a frigging multiplication! That is the formula — we said it’s simple, didn’t we? You wouldn’t imagine that a simple multiplication can be used for all kinds of incredibly useful applications, but it can. We’ll study a couple examples which will demonstrate this."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Many forms of Bayes",description:"In case you have any trouble with the following exercises, you may need to read the above material a few times and give it some time, and if that doesn´t do it, you can look for more material online. Just a word of advice: there are many different forms in which the Bayes rule can be written, and the odds form that we use isn´t the most common one. Here are a couple links that you may find useful.<ul><li><a href='https://www.youtube.com/watch?v=tRE6mKAIkno'>Maths Doctor: Bayes' Theorem and medical testing</a><li><a href='https://betterexplained.com/articles/understanding-bayes-theorem-with-ratios/'>Better Explained: Understanding Bayes Theorem With Ratios</a></ul>","<":"",note:""},children:[{type:"text",value:"\n"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec61aa06ee0000047c599c"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"The Bayes rule in practice: breast cancer screening"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Our first realistic application is a classical example of using the Bayes rule, namely medical diagnosis. This example also illustrates a common bias in dealing with uncertain information called the base-rate fallacy."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"bayes-rule-1"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"bayes-rule-2"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Consider mammographic screening for breast cancer. Using made up percentages for the sake of simplifying the numbers, let’s assume that five in 100 women have breast cancer. Suppose that if a person has breast cancer, then the mammograph test will find it 80 times out of 100. When the test comes out suggesting that breast cancer is present, we say that the result is positive, although of course there is nothing positive about this for the person being tested. (A technical way of saying this is that the sensitivity of the test is 80%.)"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The test may also fail in the other direction, namely to indicate breast cancer when none exists. This is called a false positive finding. Suppose that if the person being tested actually doesn’t have breast cancer, the chances that the test nevertheless comes out positive are 10 in 100."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Based on the above probabilities, you are be able to calculate the likelihood ratio. You'll find use for it in the next exercise. If you forgot how the likelihood ratio is calculated, you may wish to check the terminology box earlier in this section and revisit the rain example."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec628006ee0000047c599f"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]}],data:{quirksMode:!1}},excerpt:"We will do this because this particular formula is both simple and elegant as well as incredibly powerful. It can be used to weigh…",frontmatter:{path:"/3/2",title:"The Bayes Rule",part:3,type:"section",lang:"en",section:2}}},{node:{htmlAst:{type:"root",children:[{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"lead",properties:{},children:[{type:"text",value:"One of the most useful applications of the Bayes rule is the so-called naive Bayes classifier. "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The Bayes classifier is a machine learning technique that can be used to classify objects such as text documents into two or more classes. The classifier is trained by analysing a set of training data, for which the correct classes are given."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The naive Bayes classifier can be used to determine the probabilities of the classes given a number of different observations. The assumption in the model is that the feature variables are conditionally independent given the class. (We will not discuss the meaning of conditional independence in this course. For our purposes, it is enough to be able to exploit conditional independence in building the classifier.)"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Real world application: spam filters"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We will use a spam email filter as a running example for illustrating the idea of the naive Bayes classifier. Thus, the class variable indicates whether a message is spam (or “junk email”) or whether it is a legitimate message (also called “ham”). The words in the message correspond to the feature variables, so that the number of feature variables in the model is determined by the length of the message."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Why we call it “naive”",description:"Using spam filters as an example, the idea is to think of the words as being produced by choosing one word after the other so that the choice of the word depends only on whether the message is spam or ham. This is a crude simplification of the process because it means that there is no dependency between adjacent words, and the order of the words has no significance. This is in fact why the method is called naive."},children:[{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The above idea is usually depicted using the following illustration where the class of the message (spam or ham) is the only factor that has an effect on the words."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"illustrations",properties:{motive:"spam-or-ham"},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Despite it’s naivete, the naive Bayes method tends to work very well in practice. This is a good example of what the common saying in statistics, “all models are wrong, but some are useful” means. (The aphorism is generally attributed to statistician "},{type:"element",tagName:"a",properties:{href:"https://en.wikipedia.org/wiki/George_E._P._Box"},children:[{type:"text",value:"George E.P. Box"}]},{type:"text",value:".)"}]},{type:"text",value:"\n"},{type:"element",tagName:"h2",properties:{},children:[{type:"text",value:"Estimating parameters"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To get started, we need to specify the prior odds for spam (against ham). For simplicity assume this to be 1:1 which means that on the average half of the incoming messages are spam. (In reality, the amount of spam is probably much higher.)"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To get our likelihood ratios, we need two different probabilities for any word occurring: one in spam messages and another one in ham messages."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"The word distributions for the two classes are best estimated from actual training data that contains some spam messages as well as legitimate messages. The simplest way is to count how many times each word, abacus, acacia, ..., zurg, appears in the data and divide the number by the total word count."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To illustrate the idea, let’s assume that we have at our disposal some spam and some ham. You can easily obtain such data by saving a batch of your emails in two files."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Assume that we have calculated the number of occurrences of the following words (along with all other words) in the two classes of messages:"}]},{type:"text",value:"\n"},{type:"element",tagName:"table",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"tbody",properties:{},children:[{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"word"}]},{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"spam"}]},{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"ham"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"million"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"156"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"98"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"dollars"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"29"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"119"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"adclick"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"51"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"conferences"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"12"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"total"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"95791"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"306438"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We can now estimate that the probability that a word in a spam message is million, for example, is about 156 out of 95791, which is roughly the same as 1 in 614. Likewise, we get the estimate that 98 out of 306438 words, which is about the same as 1 in 3127, in a ham message are million. Both of these probability estimates are small, less than 1 in 500, but more importantly, the former is higher than the latter: 1 in 614 is higher than 1 in 3127. This means that the likelihood ratio, which is the first ratio divided by the second ratio, is more than one. To be more precise, the ratio is (1/614) / (1/3127) = 3127/614 = 5.1 (rounded to one decimal digit)."}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Recall that if you have any trouble at all with following the math in this section, you should refresh the arithmetics with fractions using the pointers we gave earlier (see the part about Odds in section "},{type:"element",tagName:"i",properties:{},children:[{type:"text",value:"Odds and Probability"}]},{type:"text",value:")."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"note",properties:{heading:"Zero means trouble",description:"One problem with estimating the probabilities directly from the counts is that the zero counts lead to zero estimates. This can be quite harmful for the performance of the classifier - it easily leads to situations where the posterior odds is 0/0, which is nonsense. The simplest solution is to use a small lower bound for all probability estimates. The value 1/100000, for instance, does the job."},children:[]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Using the above logic, we can determine the likelihood ratio for all possible words without having to use zero, giving us the following likelihood ratios:"}]},{type:"text",value:"\n"},{type:"element",tagName:"table",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"tbody",properties:{},children:[{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"word"}]},{type:"text",value:"\n    "},{type:"element",tagName:"th",properties:{},children:[{type:"text",value:"likelihood ratio"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"million"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"5.1"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"dollars"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0.8"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"adclick"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"53.2"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n  "},{type:"element",tagName:"tr",properties:{},children:[{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"conferences"}]},{type:"text",value:"\n    "},{type:"element",tagName:"td",properties:{},children:[{type:"text",value:"0.3"}]},{type:"text",value:"\n  "}]},{type:"text",value:"\n"}]}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"We are now ready to apply the method to classify new messages."}]},{type:"text",value:"\n"},{type:"element",tagName:"h3",properties:{},children:[{type:"text",value:"Example: is it spam or ham?"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Once we have the prior odds and the likelihood ratios calculated, we are ready to apply the Bayes rule, which we already practiced in the medical diagnosis case as our example. The reasoning goes just like it did before: we update the odds of spam by multiplying it by the likelihood ratio. To remind ourselves of the procedure, let's try a message with a single message to begin with. For the prior odds, as agreed above, you should use odds 1:1."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec62d906ee0000047c59a0"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"To handle the rest of the words in a message, we can use exactly the same procedure. The posterior odds after one word, which you calculated in the previous exercise, will become the prior odds for the next word, and so on."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n"},{type:"element",tagName:"quiz",properties:{quizid:"5aec630a06ee0000047c59a2"},children:[{type:"text",value:"\n"}]},{type:"text",value:"\n"}]},{type:"text",value:"\n"},{type:"element",tagName:"p",properties:{},children:[{type:"text",value:"Hooray! You have now mastered a powerful technique used every day in a wide range of real-world AI applications, the naive Bayes classifier. Even if you had to skip some of the technicalities, you should try to make sure you understood the basic principles of applying probabilities to update beliefs. As we discussed in the beginning of this Chapter, the main advantage of probabilistic reasoning is the ability to handle uncertain and conflicting evidence. Using examples in medical diagnosis and spam filtering, we demonstrated how this work is practice."}]},{type:"text",value:"\n"},{type:"element",tagName:"div",properties:{},children:[{type:"text",value:"\n  "},{type:"element",tagName:"part-summary",properties:{chapter:"3",heading:"After completing Chapter 3 you should be able to:",listitems:'[\n  {"content":"Express probabilities in terms of natural frequencies"},\n  {"content":"Apply the Bayes rule to infer risks in simple scenarios"},\n  {"content":"Explain the base-rate fallacy and avoid it by applying Bayesian reasoning"}\n    ]'},children:[{type:"text",value:">\n  "}]},{type:"text",value:"\n"}]}],data:{quirksMode:!1}},excerpt:"The Bayes classifier is a machine learning technique that can be used to classify objects such as text documents into two or more classes…",frontmatter:{path:"/3/3",title:"Naive Bayes classification",part:3,type:"section",lang:"en",section:3}}}]},allParts:{totalCount:6,edges:[{node:{frontmatter:{title:"What is AI?",path:"/1",section:null,part:1,lang:"en",bannerImage:{publicURL:"/static/banner1-5cb707dcbce557b358c736c82a82b847.png"}}}},{node:{frontmatter:{title:"AI problem solving",path:"/2",section:null,part:2,lang:"en",bannerImage:{publicURL:"/static/banner2-3217219fe81de9c2f030e51f04557962.png"}}}},{node:{frontmatter:{title:"Real world AI",path:"/3",section:null,part:3,lang:"en",bannerImage:{publicURL:"/static/banner3-8433f94cdf930cb1172a332eda51a0ae.png"}}}},{node:{frontmatter:{title:"Machine learning",path:"/4",section:null,part:4,lang:"en",bannerImage:{publicURL:"/static/banner4-fdc0e4c1dc187a976325542364658e54.png"}}}},{node:{frontmatter:{title:"Neural networks",path:"/5",section:null,part:5,lang:"en",bannerImage:{publicURL:"/static/banner5-8d6d86ca3c422d98b6213f5ddfbe8c07.png"}}}},{node:{frontmatter:{title:"Implications",path:"/6",section:null,part:6,lang:"en",bannerImage:{publicURL:"/static/banner6-2943d36053a6dd8bd40b3dc3832bb0f8.png"}}}}]},currentPart:{htmlAst:{type:"root",children:[],data:{quirksMode:!1}},frontmatter:{path:"/3",title:"Real world AI",part:3,lang:"en",quote:"One of the reasons why modern AI methods actually work in the real world - as opposed to most of the earlier good old-fashioned methods in the 1960-1980s - is the ability to deal with uncertainty.",quoteAuthor:"",bannerImage:{publicURL:"/static/banner3-8433f94cdf930cb1172a332eda51a0ae.png"}}},allSections:{totalCount:18,edges:[{node:{frontmatter:{title:"How should we define AI?",path:"/1/1",section:1,part:1,lang:"en"}}},{node:{frontmatter:{title:"Odds and probability",path:"/3/1",section:1,part:3,lang:"en"}}},{node:{frontmatter:{title:"Search and problem solving",path:"/2/1",section:1,part:2,lang:"en"}}},{node:{frontmatter:{title:"The types of machine learning",path:"/4/1",section:1,part:4,lang:"en"}}},{node:{frontmatter:{title:"About predicting the future",path:"/6/1",section:1,part:6,lang:"en"}}},{node:{frontmatter:{title:"Neural network basics",path:"/5/1",section:1,part:5,lang:"en"}}},{node:{frontmatter:{title:"Related fields",path:"/1/2",section:2,part:1,lang:"en"}}},{node:{frontmatter:{title:"The Bayes Rule",path:"/3/2",section:2,part:3,lang:"en"}}},{node:{frontmatter:{title:"Solving problems with AI",path:"/2/2",section:2,part:2,lang:"en"}}},{node:{frontmatter:{title:"The nearest neighbor classifier",path:"/4/2",section:2,part:4,lang:"en"}}},{node:{frontmatter:{title:"The societal implications of AI",path:"/6/2",section:2,part:6,lang:"en"}}},{node:{frontmatter:{title:"How neural networks are built",path:"/5/2",section:2,part:5,lang:"en"}}},{node:{frontmatter:{title:"Philosophy of AI",path:"/1/3",section:3,part:1,lang:"en"}}},{node:{frontmatter:{title:"Naive Bayes classification",path:"/3/3",section:3,part:3,lang:"en"}}},{node:{frontmatter:{title:"Search and games",path:"/2/3",section:3,part:2,lang:"en"}}},{node:{frontmatter:{title:"Summary",path:"/6/3",section:3,part:6,lang:"en"}}},{node:{frontmatter:{title:"Regression",path:"/4/3",section:3,part:4,lang:"en"}}},{node:{frontmatter:{title:"Advanced neural network techniques",path:"/5/3",section:3,part:5,lang:"en"}}}]}},pathContext:{part:3,type:"section",lang:"en"}}}});
//# sourceMappingURL=path---3-3-bc50e69169a81be664c4.js.map